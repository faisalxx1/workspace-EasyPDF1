# Robots.txt for EasyPDF Tools
# Website: https://easypdftools.com
# Last updated: 2024

User-agent: *
Allow: /
Allow: /api/

# Sitemap location
Sitemap: https://easypdftools.com/sitemap.xml

# Crawl delay to be respectful to server resources
Crawl-delay: 1

# Block common bot paths that shouldn't be indexed
Disallow: /private/
Disallow: /admin/
Disallow: /_next/
Disallow: /api/auth/
Disallow: /api/user/
Disallow: /api/download/
Disallow: /temp/
Disallow: /cache/

# Block specific file types
Disallow: /*.json$
Disallow: /*.log$

# Allow specific search engines with full access
User-agent: Googlebot
Allow: /

User-agent: Bingbot
Allow: /

User-agent: Slurp
Allow: /

User-agent: DuckDuckBot
Allow: /

User-agent: Baiduspider
Allow: /

User-agent: YandexBot
Allow: /

# Block aggressive bots and scrapers
User-agent: SemrushBot
Disallow: /

User-agent: AhrefsBot
Disallow: /

User-agent: MJ12bot
Disallow: /

User-agent: DotBot
Disallow: /

User-agent: BLEXBot
Disallow: /

User-agent: BacklinkCrawler
Disallow: /

# Special instructions for specific bots
User-agent: GPTBot
Disallow: /

User-agent: ChatGPT-User
Disallow: /

User-agent: CCBot
Disallow: /

User-agent: anthropic-ai
Disallow: /

# Allow image indexing
User-agent: Googlebot-Image
Allow: /images/
Allow: /public/

User-agent: Bingbot-Image
Allow: /images/
Allow: /public/